package ee.carlrobert.codegpt.agent.clients

import ai.koog.prompt.executor.clients.openai.base.models.*
import kotlinx.serialization.SerialName
import kotlinx.serialization.Serializable
import kotlinx.serialization.json.JsonElement

@Serializable
class CustomOpenAIChatCompletionRequest(
    val messages: List<OpenAIMessage> = emptyList(),
    val prompt: String? = null,
    override val model: String? = null,
    override val stream: Boolean? = null,
    override val temperature: Double? = null,
    val tools: List<OpenAITool>? = null,
    val toolChoice: OpenAIToolChoice? = null,
    override val topP: Double? = null,
    override val topLogprobs: Int? = null,
    val maxTokens: Int? = null,
    val frequencyPenalty: Double? = null,
    val presencePenalty: Double? = null,
    val responseFormat: OpenAIResponseFormat? = null,
    val stop: List<String>? = null,
    val logprobs: Boolean? = null,
    val seed: Int? = null,
    val topK: Int? = null,
    val repetitionPenalty: Double? = null,
    val logitBias: Map<Int, Double>? = null,
    val minP: Double? = null,
    val topA: Double? = null,
    val prediction: OpenAIStaticContent? = null,
    val transforms: List<String>? = null,
    val models: List<String>? = null,
    val route: String? = null,
    val user: String? = null,
    val additionalProperties: Map<String, JsonElement>? = null,
) : OpenAIBaseLLMRequest

/**
 * Chat completion choice
 *
 * @property finishReason The reason the model stopped generating tokens.
 * This will be `stop` if the model hit a natural stop point or a provided stop sequence,
 * `length` if the maximum number of tokens specified in the request was reached,
 * `content_filter` if content was omitted due to a flag from our content filters,
 * `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
 * `error` if the model finishes the request with an error.
 * @property nativeFinishReason The raw finish_reason string returned by the model
 * @property message A chat completion message generated by the model.
 * @property error An error response structure typically used for conveying error details to the clients.
 *
 * See (CompletionsResponse Format)[https://CustomOpenAI.ai/docs/api-reference/overview#completionsresponse-format]
 */
@Serializable
public class CustomOpenAIChoice(
    public val finishReason: String? = null,
    public val nativeFinishReason: String? = null,
    public val message: OpenAIMessage,
    public val error: ErrorResponse? = null
)

/**
 * Chat completion choice
 *
 * @property finishReason The reason the model stopped generating tokens.
 * This will be `stop` if the model hit a natural stop point or a provided stop sequence,
 * `length` if the maximum number of tokens specified in the request was reached,
 * `content_filter` if content was omitted due to a flag from our content filters,
 * `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
 * `error` if the model finishes the request with an error.
 * @property nativeFinishReason The raw finish_reason string returned by the model
 * @property delta A chat completion delta generated by streamed model responses.
 * @property error An error response structure typically used for conveying error details to the clients.
 *
 * See (CompletionsResponse Format)[https://CustomOpenAI.ai/docs/api-reference/overview#completionsresponse-format]
 */
@Serializable
public class CustomOpenAIStreamChoice(
    public val finishReason: String? = null,
    public val nativeFinishReason: String? = null,
    public val delta: CustomOpenAIStreamDelta,
    public val error: ErrorResponse? = null
)

/**
 * @property content The contents of the chunk message.
 * @property role The role of the author of this message.
 * @property toolCalls The tool calls requested by the model.
 */
@Serializable
public class CustomOpenAIStreamDelta(
    public val content: String? = null,
    public val role: String? = null,
    public val toolCalls: List<CustomOpenAIToolCall>? = null
)

@Serializable
public class CustomOpenAIToolCall(
    public val id: String? = "",
    public val index: Int? = 0,
    public val function: CustomOpenAIFunction
) {
    /** The type of the tool. Currently, only `function` is supported. */
    public val type: String = "function"
}

@Serializable
public class CustomOpenAIFunction(
    public val name: String? = "",
    public val arguments: String? = ""
)

/**
 * Represents an error response structure typically used for conveying error details to the clients.
 *
 * @property code The numeric code representing the error.
 * @property message A descriptive message providing details about the error.
 * @property metadata Optional additional information about the error in the form of key-value pairs.
 */
@Serializable
public class ErrorResponse(
    public val code: Int,
    public val message: String,
    public val metadata: Map<String, String>? = null,
)

/**
 * CustomOpenAI Chat Completion Response
 * See (CompletionsResponse Format)[https://CustomOpenAI.ai/docs/api-reference/overview#completionsresponse-format]
 */
@Serializable
public class CustomOpenAIChatCompletionResponse(
    public val choices: List<CustomOpenAIChoice>,
    override val created: Long,
    override val id: String,
    override val model: String,
    public val systemFingerprint: String? = null,
    @SerialName("object")
    public val objectType: String = "chat.completion",
    public val usage: OpenAIUsage? = null,
) : OpenAIBaseLLMResponse

/**
 * CustomOpenAI Chat Completion Streaming Response
 * See (CompletionsResponse Format)[https://CustomOpenAI.ai/docs/api-reference/overview#completionsresponse-format]
 */
@Serializable
public class CustomOpenAIChatCompletionStreamResponse(
    public val choices: List<CustomOpenAIStreamChoice>,
    override val created: Long,
    override val id: String,
    override val model: String,
    public val systemFingerprint: String? = null,
    @SerialName("object")
    public val objectType: String = "chat.completion.chunk",
    public val usage: OpenAIUsage? = null,
) : OpenAIBaseLLMStreamResponse
